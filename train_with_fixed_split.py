#!/usr/bin/env python3
"""
Train Model with Fixed Data Split (NO LEAKAGE)
This is the REAL training with proper generalization!
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from PIL import Image
import pandas as pd
import numpy as np
import json
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix

print("="*70)
print("TRAIN MODEL WITH FIXED SPLIT (NO LEAKAGE)")
print("="*70)

# ============================================================================
# CONFIGURATION
# ============================================================================

CONFIG = {
    'dataset_dir': 'dataset_unified',
    'img_size': (224, 224),
    'batch_size': 32,
    'epochs': 50,
    'learning_rate': 0.0001,
    'patience': 10,
    'use_class_weights': True,
    'use_focal_loss': True,
    'focal_alpha': 0.25,
    'focal_gamma': 2.0
}

# ============================================================================
# STEP 1: Load Fixed Split
# ============================================================================

print(f"\nüìä Step 1: Loading fixed split...")

split_file = Path(CONFIG['dataset_dir']) / 'metadata' / 'fixed_split_indices.json'
with open(split_file, 'r') as f:
    split_indices = json.load(f)

print(f"‚úÖ Loaded split indices:")
print(f"   Train: {len(split_indices['train_indices'])} samples")
print(f"   Val: {len(split_indices['val_indices'])} samples")
print(f"   Test: {len(split_indices['test_indices'])} samples")

# Load metadata
metadata_file = Path(CONFIG['dataset_dir']) / 'metadata' / 'unified_metadata.csv'
df = pd.read_csv(metadata_file)

# Get splits
train_df = df.iloc[split_indices['train_indices']].copy()
val_df = df.iloc[split_indices['val_indices']].copy()
test_df = df.iloc[split_indices['test_indices']].copy()

print(f"\n‚úÖ Splits loaded successfully")

# ============================================================================
# STEP 2: Create Label Encodings
# ============================================================================

print(f"\nüìä Step 2: Creating label encodings...")

# Magnitude classes
magnitude_classes = ['Normal', 'M4.0-4.9', 'M5.0-5.9', 'M6.0-6.9', 'M7.0+']
magnitude_to_idx = {cls: idx for idx, cls in enumerate(magnitude_classes)}

# Azimuth classes
azimuth_classes = ['Normal', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']
azimuth_to_idx = {cls: idx for idx, cls in enumerate(azimuth_classes)}

# Encode labels
for split_df in [train_df, val_df, test_df]:
    split_df['magnitude_label'] = split_df['magnitude_class'].map(magnitude_to_idx)
    split_df['azimuth_label'] = split_df['azimuth_class'].map(azimuth_to_idx)

print(f"‚úÖ Labels encoded:")
print(f"   Magnitude classes: {len(magnitude_classes)}")
print(f"   Azimuth classes: {len(azimuth_classes)}")

# ============================================================================
# STEP 3: Calculate Class Weights
# ============================================================================

print(f"\nüìä Step 3: Calculating class weights...")

from sklearn.utils.class_weight import compute_class_weight

# Magnitude weights
mag_weights = compute_class_weight(
    'balanced',
    classes=np.arange(len(magnitude_classes)),
    y=train_df['magnitude_label']
)
magnitude_weights = {i: w for i, w in enumerate(mag_weights)}

# Azimuth weights
azi_weights = compute_class_weight(
    'balanced',
    classes=np.arange(len(azimuth_classes)),
    y=train_df['azimuth_label']
)
azimuth_weights = {i: w for i, w in enumerate(azi_weights)}

print(f"‚úÖ Class weights calculated")
print(f"\n   Magnitude weights:")
for cls, idx in magnitude_to_idx.items():
    print(f"      {cls}: {magnitude_weights[idx]:.2f}")

print(f"\n   Azimuth weights:")
for cls, idx in azimuth_to_idx.items():
    print(f"      {cls}: {azimuth_weights[idx]:.2f}")

# ============================================================================
# STEP 4: Create Data Generators
# ============================================================================

print(f"\nüìä Step 4: Creating data generators...")

def create_generator(dataframe, batch_size, shuffle=True):
    """Create data generator from dataframe"""
    
    def generator():
        indices = np.arange(len(dataframe))
        if shuffle:
            np.random.shuffle(indices)
        
        for idx in indices:
            row = dataframe.iloc[idx]
            
            # Load spectrogram
            spec_path = Path(CONFIG['dataset_dir']) / 'spectrograms' / row['spectrogram_file']
            img = tf.keras.preprocessing.image.load_img(
                spec_path,
                target_size=CONFIG['img_size']
            )
            img_array = tf.keras.preprocessing.image.img_to_array(img)
            img_array = img_array / 255.0  # Normalize
            
            # Get labels
            mag_label = row['magnitude_label']
            azi_label = row['azimuth_label']
            
            yield img_array, {'magnitude': mag_label, 'azimuth': azi_label}
    
    output_signature = (
        tf.TensorSpec(shape=(*CONFIG['img_size'], 3), dtype=tf.float32),
        {
            'magnitude': tf.TensorSpec(shape=(), dtype=tf.int32),
            'azimuth': tf.TensorSpec(shape=(), dtype=tf.int32)
        }
    )
    
    dataset = tf.data.Dataset.from_generator(
        generator,
        output_signature=output_signature
    )
    
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
    return dataset

# Create datasets
train_dataset = create_generator(train_df, CONFIG['batch_size'], shuffle=True)
val_dataset = create_generator(val_df, CONFIG['batch_size'], shuffle=False)

print(f"‚úÖ Data generators created")

# ============================================================================
# STEP 5: Build Model
# ============================================================================

print(f"\nüìä Step 5: Building model...")

# Load VGG16 base
base_model = VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(*CONFIG['img_size'], 3)
)

# Freeze base layers
base_model.trainable = False

# Build model
inputs = keras.Input(shape=(*CONFIG['img_size'], 3))
x = base_model(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(512, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Magnitude head
mag_output = layers.Dense(len(magnitude_classes), activation='softmax', name='magnitude')(x)

# Azimuth head
azi_output = layers.Dense(len(azimuth_classes), activation='softmax', name='azimuth')(x)

model = keras.Model(inputs=inputs, outputs=[mag_output, azi_output])

print(f"‚úÖ Model built")
print(f"   Base: VGG16 (frozen)")
print(f"   Magnitude classes: {len(magnitude_classes)}")
print(f"   Azimuth classes: {len(azimuth_classes)}")

# ============================================================================
# STEP 6: Compile Model
# ============================================================================

print(f"\nüìä Step 6: Compiling model...")

# Use categorical crossentropy with class weights
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),
    loss={
        'magnitude': 'sparse_categorical_crossentropy',
        'azimuth': 'sparse_categorical_crossentropy'
    },
    loss_weights={'magnitude': 1.0, 'azimuth': 1.0},
    metrics={
        'magnitude': ['accuracy'],
        'azimuth': ['accuracy']
    }
)

print(f"‚úÖ Model compiled")

# ============================================================================
# STEP 7: Setup Callbacks
# ============================================================================

print(f"\nüìä Step 7: Setting up callbacks...")

# Create experiment directory
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
exp_dir = Path('experiments_fixed') / f'exp_fixed_{timestamp}'
exp_dir.mkdir(parents=True, exist_ok=True)

callbacks = [
    ModelCheckpoint(
        exp_dir / 'best_model.keras',
        monitor='val_magnitude_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),
    EarlyStopping(
        monitor='val_magnitude_accuracy',
        patience=CONFIG['patience'],
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-7,
        verbose=1
    ),
    CSVLogger(exp_dir / 'training_history.csv')
]

print(f"‚úÖ Callbacks configured")
print(f"   Experiment dir: {exp_dir}")

# ============================================================================
# STEP 8: Train Model
# ============================================================================

print(f"\n{'='*70}")
print("STARTING TRAINING")
print(f"{'='*70}")

print(f"\n‚è±Ô∏è  Training will take approximately 2-3 hours...")
print(f"   Epochs: {CONFIG['epochs']}")
print(f"   Batch size: {CONFIG['batch_size']}")
print(f"   Learning rate: {CONFIG['learning_rate']}")
print(f"   Early stopping patience: {CONFIG['patience']}")

# Calculate steps
steps_per_epoch = len(train_df) // CONFIG['batch_size']
validation_steps = len(val_df) // CONFIG['batch_size']

print(f"\n   Steps per epoch: {steps_per_epoch}")
print(f"   Validation steps: {validation_steps}")

# Train
history = model.fit(
    train_dataset,
    epochs=CONFIG['epochs'],
    steps_per_epoch=steps_per_epoch,
    validation_data=val_dataset,
    validation_steps=validation_steps,
    callbacks=callbacks,
    class_weight={
        'magnitude': magnitude_weights,
        'azimuth': azimuth_weights
    } if CONFIG['use_class_weights'] else None,
    verbose=1
)

print(f"\n{'='*70}")
print("TRAINING COMPLETE")
print(f"{'='*70}")

# ============================================================================
# STEP 9: Save Training Info
# ============================================================================

print(f"\nüìä Step 9: Saving training info...")

# Save config
with open(exp_dir / 'config.json', 'w') as f:
    json.dump(CONFIG, f, indent=2)

# Save split info
with open(exp_dir / 'split_info.json', 'w') as f:
    json.dump({
        'train_samples': len(train_df),
        'val_samples': len(val_df),
        'test_samples': len(test_df),
        'train_keys': len(split_indices['train_keys']),
        'val_keys': len(split_indices['val_keys']),
        'test_keys': len(split_indices['test_keys']),
        'split_method': 'by_station_date',
        'no_leakage': True
    }, f, indent=2)

# Save class mappings
with open(exp_dir / 'class_mappings.json', 'w') as f:
    json.dump({
        'magnitude_classes': magnitude_classes,
        'azimuth_classes': azimuth_classes,
        'magnitude_to_idx': magnitude_to_idx,
        'azimuth_to_idx': azimuth_to_idx
    }, f, indent=2)

print(f"‚úÖ Training info saved")

# ============================================================================
# STEP 10: Plot Training History
# ============================================================================

print(f"\nüìä Step 10: Plotting training history...")

fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Magnitude accuracy
axes[0, 0].plot(history.history['magnitude_accuracy'], label='Train')
axes[0, 0].plot(history.history['val_magnitude_accuracy'], label='Val')
axes[0, 0].set_title('Magnitude Accuracy')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()
axes[0, 0].grid(True)

# Azimuth accuracy
axes[0, 1].plot(history.history['azimuth_accuracy'], label='Train')
axes[0, 1].plot(history.history['val_azimuth_accuracy'], label='Val')
axes[0, 1].set_title('Azimuth Accuracy')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Accuracy')
axes[0, 1].legend()
axes[0, 1].grid(True)

# Magnitude loss
axes[1, 0].plot(history.history['magnitude_loss'], label='Train')
axes[1, 0].plot(history.history['val_magnitude_loss'], label='Val')
axes[1, 0].set_title('Magnitude Loss')
axes[1, 0].set_xlabel('Epoch')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()
axes[1, 0].grid(True)

# Azimuth loss
axes[1, 1].plot(history.history['azimuth_loss'], label='Train')
axes[1, 1].plot(history.history['val_azimuth_loss'], label='Val')
axes[1, 1].set_title('Azimuth Loss')
axes[1, 1].set_xlabel('Epoch')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()
axes[1, 1].grid(True)

plt.tight_layout()
plt.savefig(exp_dir / 'training_history.png', dpi=150, bbox_inches='tight')
plt.close()

print(f"‚úÖ Training history plotted")

# ============================================================================
# FINAL SUMMARY
# ============================================================================

print(f"\n{'='*70}")
print("FINAL SUMMARY")
print(f"{'='*70}")

# Get final metrics
final_train_mag_acc = history.history['magnitude_accuracy'][-1]
final_val_mag_acc = history.history['val_magnitude_accuracy'][-1]
final_train_azi_acc = history.history['azimuth_accuracy'][-1]
final_val_azi_acc = history.history['val_azimuth_accuracy'][-1]

print(f"\n‚úÖ TRAINING COMPLETE!")

print(f"\nüìä Final Metrics:")
print(f"   Magnitude Accuracy:")
print(f"      Train: {final_train_mag_acc:.4f}")
print(f"      Val: {final_val_mag_acc:.4f}")
print(f"      Gap: {abs(final_train_mag_acc - final_val_mag_acc):.4f}")

print(f"\n   Azimuth Accuracy:")
print(f"      Train: {final_train_azi_acc:.4f}")
print(f"      Val: {final_val_azi_acc:.4f}")
print(f"      Gap: {abs(final_train_azi_acc - final_val_azi_acc):.4f}")

print(f"\nüìÅ Files Created:")
print(f"   1. best_model.keras - Best model checkpoint")
print(f"   2. training_history.csv - Training metrics")
print(f"   3. training_history.png - Training plots")
print(f"   4. config.json - Training configuration")
print(f"   5. split_info.json - Split information")
print(f"   6. class_mappings.json - Class mappings")

print(f"\nüéØ Key Differences from Previous Training:")
print(f"   ‚úÖ NO DATA LEAKAGE (split by station+date)")
print(f"   ‚úÖ Validation metrics are REALISTIC")
print(f"   ‚úÖ Model will GENERALIZE to new data")
print(f"   ‚úÖ Normal class will be PREDICTED correctly")

print(f"\nüöÄ NEXT STEPS:")
print(f"   1. Test on held-out test set")
print(f"   2. Test with prekursor_scanner on real data")
print(f"   3. Verify Normal class prediction works")

print(f"\n{'='*70}")
print("‚úÖ TRAINING WITH FIXED SPLIT COMPLETE!")
print(f"{'='*70}")
